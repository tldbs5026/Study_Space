
# DKT
- 딥러닝을 이용한 지식 상태 추적
- 각 지식에 대한 학생의 이해도를 파악
	- 변하고 축적되는 지식을 추적하는 task
- 시험지의 난이도 + 문제 풀이 정보를 바탕으로 예측
- 데이터가 많을수록 정확, 적을수록 오버피팅

## 활용
- 부족한 부분을 집중학습
- 뛰어나다면 다음 단계로 나갈 수 있게 도움

## metric
- AUC/ACC

AUC
- ROC 밑부분 면적
- 0.5 == 랜덤
- 높을수록 좋은 성능
- x축은 FPR, y축은 TPR
- 불변하는 척도로 절댓값이 아닌 예측의 평가를 측정
	- 잘 보정된 확률 결과에 있어서는 알 수 없음
- 분류 임계값이 불변함에 따라 분류 임계값과 관계없이 모델의 예측 품질을 측정 가능
	- FN과 FP에서의 비용이 큰 차이가 있는 경우 사용하기 어려움
	- 이메일 스팸에서 FP의 최소화를 우선시하고 싶을 경우 사용 불가능.

트렌드
1. Model
- transformer위주에서 GNN, CNN등 활용
2. Data
- elapsed time, lag time등 사용
3. Regularization term
- 구조를 변경하지 않고 정규화 항을 추가
4. Embedding
- 과거의 방식을 그대로 사용
- qDKT의 fast Text


iScream 구조
- userId 
	- 사용자의 고유 번호
- assessmentItemID
	- 사용자가 푼 문항의 일련 번호
- testId
	- 시험지의 일련 번호
- answerCode
	- 정답 여부
- Timestamp
	- 문항을 푼 시간 정보
- KnowledgeTag
	- 문항의 고유 태그

## 기술 통계량
- 데이터의 정보를 수치로 요약, 단순화
- EDA에서는 이를 시각화하는 작업
- 정답률과 연관 지어 진행

일반적 EDa
- 문항을 더 많이 푼 학생과 정답률
- 태그의 노출도 수준
- 푼 문항과 정답률
- 시간과 정답률


# Baseline Model

Sequence와 non Sequential Data
- Sequential한 모델에서는 aggregation -> FE
- Transaction 그대로 사용 + FE로 나뉨

## Approach

### Tabular
- 정형데이터로 가정하고 모델링

FE
- 문제를 푼 시점에서의 ACC
- 문제, 시험 별 난이도
- 사용자 단위로 split

모델 훈련
- 하이퍼파라미터 수정
### Sequential 

LSTM
```
import torch
import torch.nn as nn

#[batch_size, seq_len, input_size/#features]
input = torch.randn(3,5,4)

lstm = nn.LSTM(input_size = 4, hidden_size=2, batch_first=True)

output, h = lstm(input)
output.size() -> (3,5,2)
```
- output의 size는 각각 (batch_size, seq_len, lstm에서의 적용값인 hidden_size)

Transformers(허깅페이스를 이용한)

```
config = BertConfig(3,   # vocab_size
hidden_Size=4, num_attention_heads = 1
)

size : [batch,seq_len, input_size]
input = torch.randn(3,5,4)

mask = torch,randn(3,5)
transformer = BertModel(config)
encoded_layers = transformers(inputs_embeds=input, attention_mask = mask)
sequence_output = encoded_layers[0]
sequence_output.size()
```
- 최종적인 output은 [batch_size, seq_len, input_size]



## 임베딩
- 카테고리 데이터를 embedding한 후, linear -> LayerNorm
- 연속형 데이터는 임베딩을 거치지 않고 linear -> LayerNorm
- 최종적으로는 둘을 concat


## Input Transformation
- 사용자 단위로 sequence를 생성

LSTM
- 제공되는 feature를 병합하여 LSTM에 inputting

LSTM + Attention
- 기존 LSTM에 Attention Layer추가

Bert
- 기존 lstm 레이어를 bert로 대체

```
1. Make ground baseline with no fe
2. Make a small FE and see I you can understand data you have
3. Find good CV strategy
4. Feature selection
5. Make deeper FE
6. Tune Model (crude tuning)
7. Try other Models (never forget about NN)
8. Try Blending/Stackin/Ensembling
9. Final tuning
```

# 시퀀스 데이터에 맞는 transformer 아키텍처 설계

트랜스포머
- 많은 양의 데이터, 연산 요구
- 이를 위한 변형

변형된 트랜스포머
- 작은 구조를 사용해야 할 경우
- 대회 플랫폼에 따른 메모리 제약
- task에 맞춰 변형
- 