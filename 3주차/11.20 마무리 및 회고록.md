
# 1. 오늘 마무리 지어야 할 일
- ~~지난 주 과제 복습
- 찾아놓은 논문 다시 읽어보기



# 2. 내일 해야할 일
- 일단 생각해 놓은 것은 7장까지 + visualization 2강까지
- 과제 도 해당파트까지 하면 아마 3~4?


# 3. 오늘 들은 강의 정리

# 역사
딥러닝의 key components
- 데이터 : 모델이 참고하고 학습하는 자료
- 모델 : 데이터를 학습
- loss : 실제값과 학습결과 간의 차이
- 알고리즘 : Loss를 최소화 하기 위한 알고리즘

데이터
- 풀어야 하는 문제의 타입
	- classification : 주어진 물체 분류
	- Sementic Segmentation : 경계선 분류
	- Detection : 바운딩 박스
	- Pose Estimation : 2 or 3차원의 스켈레톤 확인
	- Visual Q&A : 이미지가 주어질 때 주어진 질문에 대답

모델
- 이미지, 텍스트 등의 문제가 주어질 때 답을 도출하기 위해 데이터를 변화
- 모델의 성질에 따라 결과가 달라진다.
![[제목 없음.png]]

Loss
- 모델과 데이터가 정해질 때 학습하는 방식
- 각각의 task에 따라 계산이 달라짐
	- Regression : 출력값과 target과의 제곱의 평균(MSE) 최소화
	- Classification : 주어진 출력값과 라벨 데이터 값의 CE 최소화
	- Probability : 주어진 값의 평균, 분산으로 만들어진 분포와 실제분포간의 차이 최소화(MLE) 
- Loss가 무조건 줄어든다고 원하는 값을 얻는건 아님.

optimization
- Loss만을 줄이는 것이 학습이 아님.
	- 학습하지 않은 데이터에서 잘 동작하기 위해서는 추가적인 알고리즘이 필요
- SGD
- Momentum
- NAG
- Adagrad


## Historical Review

Alexnet
![[제목 없음 1.png]]

DQN
- 딥 마인드에서 벽돌부수기 게임을 학습하기 위해 만듬.
- 강화학습
Encoder/Decoder
- 문장이 주어질 때 그 문장의 연속을 다른 문제,언어 등에 매칭
Adam
- 모멘텀 + SGD
- 모든 task에서 Adam이 좋지는 않지만, 일반적인 상황에서 좋다는 실험적 결과
GAN
- 이미지 생성자와 감별자를 학습시킴
- 생성자는 원하는 이미지를 생성하도록,
- 감별자는 생성자가 만든 이미지를 최대한 감지하도록
Residual networks
- 네트워크를 깊게 쌓도록
Transformer
- MHA
- Positional Encoding
Bert
- 트랜스포머의 Bidrectional Encoder
- fine-tuned NLP model
Big Language Model
- gpt와 같은 대규모 파라미터로 이루진 대규모 언어모델
Self Supervised Learning
- SimCLR
- 분류에서 한정된 학습데이터에서 주어진 학습데이터 이외 unlabeled된 데이터를 사용
# MLP
정의
- Neural networks are function approximators that stack affine
- transformations followed by nonlinear transformations.
	- 행렬곱에 비선형 변화(활성화 함수)이 반복적으로 일어나는 모델의 함수의 근사

선형
- 기울기, 절편을 찾는 문제
- Data : 1차원 x,y
- model : wx+b
- Loss : MSE를 감소시켜 model의 결과와 실제 값의 차이를 줄임
- 선형 회귀에서 loss를 줄이는 w,b를 찾는 것이 목적
- Loss를 w,b에 대해 각각 편미분한 결과를 찾는다.
- ![[제목 없음 2.png]]
- ![[제목 없음 3.png]]
- 각각의 결과로 업데이트
- ![[제목 없음 4.png]]
	- - 화살표는 업데이트를 의미
	- :=와 같이 표기
	- eta : Stepsize, epoch
	- eta가 너무 작거나 크게되면 학습이 진행되지 않음

- 차원을 변경시키고 싶으면 행렬을 사용(affine transformation)
	- ![[제목 없음 5.png]]
- 선형 회귀에서 w,b를 계속 쌓기만하면 결국 layer 1인 모델을 생성하는 것과 같다.
- 그렇기에 비선형 변환을 해줌으로써 원하는 정답을 얻기 위해 값을 변화시키는 것.

활성화 함수
1. ReLU
2. Sigmoid
3. Hyperbolic Tanh.

MLP
- 각 선형 변화에 비선형 변환을 취한 것을 계속 쌓는 형태의 아키텍쳐.
- Loss 계산
	- 선형 : MSE
		- true와 target의 차이에 대한 분산의 평균
	- cross entropy
		- d개의 레이블이 존재할때 가장 출력값이 높은 인덱스만 반환하고 싶다.
		- 그렇기 때문에 출력값의 실제 수는 상관없기 때문에 로짓을 취함
	- Prob : MLE

# optimization
Gradient Descent
- loss에 대한 first order partial derivatives를 구한 후 parameter에 빼주는 방법
- 그 결과 parameter에 대한 optima를 찾고자 하는 방식.

Generalization
- Training error와 Test error간의 차이

Fitting
- Under : 적은 training, 작은 데이터로 인해 학습이 일어나지 않음
- Over  : 너무 많은 training, 너무 많은 데이터로 인해 학습은 잘되나, 테스트에서 작동하지 않는 경우
	- ![[제목 없음 6.png]]

Cross Validation
- K-Fold
- 일반적으로 학습 시 Train과 test를 분리
	- 데이터를 k개의 파티션으로 나눈 후 k-1개를 train, 1개를 test로 사용

Bias and Variance
- Bias : True Target에 얼마나 근접한가
- Variance : 학습결과의 분포도
- Bias, Variance는 각각 Tradeoff관계

Bootstrapping
- 랜덤샘플링의 방법

bagging
- 전체 데이터를 쪼개서 다양한 모델을 적용시킨 후 결과의 평균 
boosting
- focuses on those specific training samples that are hard to classify.
- weak learner를 여러 개 만들어 strong한 모델을 생성하는 것이 목적

경사하강 방법
1. SGD : 한 샘플을 랜덤으로 뽑아 gradient 학습
2. 미니배치 : 배치단위로 gradient 업데이트
3. 배치 : 모든 데이터 단위로 gradient 업데이트

미니배치와 배치의 차이
- 미니배치는 Flat Minimum 으로 converge
- 배치는 Sharp Minumum으로 converge
- ![[제목 없음 7.png]]

## Gradient Descent 방법
- loss function 정의한 후 미분을 계산

SGD
- Weight에 lr*구한Gradient값을 빼줌으로써 업데이트
- lr를 적절히 잡는 문제
- W(t+1)<−Wt−η∗gt

Momentum
- 미니 배치의 결과를 활용
- ![[제목 없음 15.png]]


NAG
- 모멘텀 : 현재 parameter를 바탕으로 gradient를 계산
- NAG : t+1으로 이동 후 gradient 계산한 결과를 accumulation
- ![[제목 없음 14.png]]

Adagrad
- gradient가 커지면 분모가 커지기 때문에 다음에 적게 업데이트
- gradient가 작아지면 분모가 작아지기 때문에 다음에 크게 업데이트
- 학습이 계속 진행되면 결국 0에 수렴되는 문제
- ![[제목 없음 13.png]]

Adadelta
- window 사이즈의 변화
- learning rate가 없음
- ![[제목 없음 12.png]]

RMSprop
- Adadelta의 G를 가져옴과 동시에
- 분자에 eta를 집어넣음

Adam
- 일반적으로 가장 무난하게 좋음
- EMA of gradient squares를 가져감과 동시에 모멘텀을 적용
- beta1 : 모멘텀 유지
- beta2 : gradient squares에 대한 EMA 정보
- epsilon
- eta
- ![[제목 없음 11.png]]


## Regularization
- Generalization 성능을 높이기 위함
- 학습을 방해하기 위한 도구

Early Stopping
- 일반적으로 test를 사용하는 것보다는 validation을 사용
- loss가 커지기 시작하면 멈춤

Parameter Norm Penalty
- 함수의 공간에서 최대한 부드러운 함수만들기
    - 부드러운 함수 == 높은 generalization
- weight decay라고 불리기도함
- ![[제목 없음 10.png]]

Data Augmentation
- 데이터가 많을 수록 학습 효과가 좋음
- 주어진 한정된 데이터를 변형시켜 데이터셋의 개수를 증가시킴

Noise Robustness
- 노이즈 추가

Label Smoothing
- decision bound를 부드럽게
- mix up : 선택된 train data를 섞음
    - label을 섞음
- cut mix : 두 데이터를 일정 비율 섞음.
    - 특정 영역 다른 데이터로 대체
- ![[제목 없음 9.png]]

Dropout
- 일정비율 탈락

Batch Normalization
- 각 배치의 statistics를 정규화해줌으로써 배치의 값의 범위를 제한

Group Normalization
- Batch Norm이 모든 배치를 정규화
- 그룹은 일정 범위에 대해서만 정규화
- ![[제목 없음 8.png]]








# 4. 회고록
- 주말에 이해하지 못한부분을 다시 확인하니 그때보다 더 많은 이해가 된 것 같다. 제대로 안될때도 있지만, 좌절하지 않고 계속 반복하다보면 언젠가는 이해하게 된다는 것을 새삼 깨닫게 되었다. 비록 피어세션 때 좌절감때문에 아무것도 듣지 못해 정리가 이상하게 되긴 했지만, 무언가는 해결되었고, 다음에 더 잘하면 되는 것이기 때문에 지금하는 방식을 좀 더 유지하다보면 여유도 생길 수 있을거라 생각한다.