# 역사
딥러닝의 key components
- 데이터 : 모델이 참고하고 학습하는 자료
- 모델 : 데이터를 학습
- loss : 실제값과 학습결과 간의 차이
- 알고리즘 : Loss를 최소화 하기 위한 알고리즘

데이터
- 풀어야 하는 문제의 타입
	- classification : 주어진 물체 분류
	- Sementic Segmentation : 경계선 분류
	- Detection : 바운딩 박스
	- Pose Estimation : 2 or 3차원의 스켈레톤 확인
	- Visual Q&A : 이미지가 주어질 때 주어진 질문에 대답

모델
- 이미지, 텍스트 등의 문제가 주어질 때 답을 도출하기 위해 데이터를 변화
- 모델의 성질에 따라 결과가 달라진다.
![[제목 없음.png]]

Loss
- 모델과 데이터가 정해질 때 학습하는 방식
- 각각의 task에 따라 계산이 달라짐
	- Regression : 출력값과 target과의 제곱의 평균(MSE) 최소화
	- Classification : 주어진 출력값과 라벨 데이터 값의 CE 최소화
	- Probability : 주어진 값의 평균, 분산으로 만들어진 분포와 실제분포간의 차이 최소화(MLE) 
- Loss가 무조건 줄어든다고 원하는 값을 얻는건 아님.

optimization
- Loss만을 줄이는 것이 학습이 아님.
	- 학습하지 않은 데이터에서 잘 동작하기 위해서는 추가적인 알고리즘이 필요
- SGD
- Momentum
- NAG
- Adagrad


## Historical Review

Alexnet
![[제목 없음 1.png]]

DQN
- 딥 마인드에서 벽돌부수기 게임을 학습하기 위해 만듬.
- 강화학습
Encoder/Decoder
- 문장이 주어질 때 그 문장의 연속을 다른 문제,언어 등에 매칭
Adam
- 모멘텀 + SGD
- 모든 task에서 Adam이 좋지는 않지만, 일반적인 상황에서 좋다는 실험적 결과
GAN
- 이미지 생성자와 감별자를 학습시킴
- 생성자는 원하는 이미지를 생성하도록,
- 감별자는 생성자가 만든 이미지를 최대한 감지하도록
Residual networks
- 네트워크를 깊게 쌓도록
Transformer
- MHA
- Positional Encoding
Bert
- 트랜스포머의 Bidrectional Encoder
- fine-tuned NLP model
Big Language Model
- gpt와 같은 대규모 파라미터로 이루진 대규모 언어모델
Self Supervised Learning
- SimCLR
- 분류에서 한정된 학습데이터에서 주어진 학습데이터 이외 unlabeled된 데이터를 사용
# MLP
정의
- Neural networks are function approximators that stack affine
- transformations followed by nonlinear transformations.
	- 행렬곱에 비선형 변화(활성화 함수)이 반복적으로 일어나는 모델의 함수의 근사

선형
- 기울기, 절편을 찾는 문제
- Data : 1차원 x,y
- model : wx+b
- Loss : MSE를 감소시켜 model의 결과와 실제 값의 차이를 줄임
- 선형 회귀에서 loss를 줄이는 w,b를 찾는 것이 목적
- Loss를 w,b에 대해 각각 편미분한 결과를 찾는다.
- ![[제목 없음 2.png]]
- ![[제목 없음 3.png]]
- 각각의 결과로 업데이트
- ![[제목 없음 4.png]]
	- - 화살표는 업데이트를 의미
	- :=와 같이 표기
	- eta : Stepsize, epoch
	- eta가 너무 작거나 크게되면 학습이 진행되지 않음

- 차원을 변경시키고 싶으면 행렬을 사용(affine transformation)
	- ![[제목 없음 5.png]]
- 선형 회귀에서 w,b를 계속 쌓기만하면 결국 layer 1인 모델을 생성하는 것과 같다.
- 그렇기에 비선형 변환을 해줌으로써 원하는 정답을 얻기 위해 값을 변화시키는 것.

활성화 함수
1. ReLU
2. Sigmoid
3. Hyperbolic Tanh.

MLP
- 각 선형 변화에 비선형 변환을 취한 것을 계속 쌓는 형태의 아키텍쳐.
- Loss 계산
	- 선형 : MSE
		- true와 target의 차이에 대한 분산의 평균
	- cross entropy
		- d개의 레이블이 존재할때 가장 출력값이 높은 인덱스만 반환하고 싶다.
		- 그렇기 때문에 출력값의 실제 수는 상관없기 때문에 로짓을 취함
	- Prob : MLE

# optimization
Gradient Descent
- loss에 대한 first order partial derivatives를 구한 후 parameter에 빼주는 방법
- 그 결과 parameter에 대한 optima를 찾고자 하는 방식.

Generalization
- Training error와 Test error간의 차이

Fitting
- Under : 적은 training, 작은 데이터로 인해 학습이 일어나지 않음
- Over  : 너무 많은 training, 너무 많은 데이터로 인해 학습은 잘되나, 테스트에서 작동하지 않는 경우
	- ![[제목 없음 6.png]]

Cross Validation
- K-Fold
- 일반적으로 학습 시 Train과 test를 분리
	- 데이터를 k개의 파티션으로 나눈 후 k-1개를 train, 1개를 test로 사용

Bias and Variance
- Bias : True Target에 얼마나 근접한가
- Variance : 학습결과의 분포도
- Bias, Variance는 각각 Tradeoff관계

Bootstrapping
- 랜덤샘플링의 방법

bagging
- 전체 데이터를 쪼개서 다양한 모델을 적용시킨 후 결과의 평균 
boosting
- focuses on those specific training samples that are hard to classify.
- weak learner를 여러 개 만들어 strong한 모델을 생성하는 것이 목적

경사하강 방법
1. SGD : 한 샘플을 랜덤으로 뽑아 gradient 학습
2. 미니배치 : 배치단위로 gradient 업데이트
3. 배치 : 모든 데이터 단위로 gradient 업데이트

미니배치와 배치의 차이
- 미니배치는 Flat Minimum 으로 converge
- 배치는 Sharp Minumum으로 converge
- ![[제목 없음 7.png]]

## Gradient Descent 방법
- loss function 정의한 후 미분을 계산

SGD
- Weight에 lr*구한Gradient값을 빼줌으로써 업데이트
- lr를 적절히 잡는 문제
- W(t+1)<−Wt−η∗gt

Momentum
- 미니 배치의 결과를 활용
- ![[제목 없음 15.png]]


NAG
- 모멘텀 : 현재 parameter를 바탕으로 gradient를 계산
- NAG : t+1으로 이동 후 gradient 계산한 결과를 accumulation
- ![[제목 없음 14.png]]

Adagrad
- gradient가 커지면 분모가 커지기 때문에 다음에 적게 업데이트
- gradient가 작아지면 분모가 작아지기 때문에 다음에 크게 업데이트
- 학습이 계속 진행되면 결국 0에 수렴되는 문제
- ![[제목 없음 13.png]]

Adadelta
- window 사이즈의 변화
- learning rate가 없음
- ![[제목 없음 12.png]]

RMSprop
- Adadelta의 G를 가져옴과 동시에
- 분자에 eta를 집어넣음

Adam
- 일반적으로 가장 무난하게 좋음
- EMA of gradient squares를 가져감과 동시에 모멘텀을 적용
- beta1 : 모멘텀 유지
- beta2 : gradient squares에 대한 EMA 정보
- epsilon
- eta
- ![[제목 없음 11.png]]


## Regularization
- Generalization 성능을 높이기 위함
- 학습을 방해하기 위한 도구

Early Stopping
- 일반적으로 test를 사용하는 것보다는 validation을 사용
- loss가 커지기 시작하면 멈춤

Parameter Norm Penalty
- 함수의 공간에서 최대한 부드러운 함수만들기
    - 부드러운 함수 == 높은 generalization
- weight decay라고 불리기도함
- ![[제목 없음 10.png]]

Data Augmentation
- 데이터가 많을 수록 학습 효과가 좋음
- 주어진 한정된 데이터를 변형시켜 데이터셋의 개수를 증가시킴

Noise Robustness
- 노이즈 추가

Label Smoothing
- decision bound를 부드럽게
- mix up : 선택된 train data를 섞음
    - label을 섞음
- cut mix : 두 데이터를 일정 비율 섞음.
    - 특정 영역 다른 데이터로 대체
- ![[제목 없음 9.png]]

Dropout
- 일정비율 탈락

Batch Normalization
- 각 배치의 statistics를 정규화해줌으로써 배치의 값의 범위를 제한

Group Normalization
- Batch Norm이 모든 배치를 정규화
- 그룹은 일정 범위에 대해서만 정규화
- ![[제목 없음 8.png]]

# 데이터의 시각화

- 데이터를 그래픽 요소로 매핑하여 시각적으로 표현

1. 데이터 : 데이터의 타입 파악
2. 그래픽 : 데이터의 타입에 맞는 그래픽 선택
3. 매핑 : 데이터에 맞는 그래픽화 매칭
4. 시각 : 시각화의 중요성

시각화에 포함된 Task
1. 목적 : 시각화 하는 목적
2. 독자 : 시각화 결과를 보는 대상
3. 데이터 : 어떤 데이터?
4. 스토리 : 어떤 흐름
5. 방법 : 전달하고자 하는 내용에 맞게
6. 디자인 : UI

# 시각화 요소

## 데이터 이해

시각화를 진행할 데이터
- 데이터셋 관점
    - 분포
- 개별 데이터의 관점

데이터셋
- 정형 : 테이블, csv로 제공
    - 통계적 특성, feature 사이
    - 데이터 간 관계
    - 데이터 간 비교
- 시계열 : 시간 흐름에 따른 데이터
    - 기온, 주가 등의 정형데이터
    - 음성, 비디오 등의 비정형
    - 시간 흐름에 따른 추세, 계절, 주기성 파악
- 지리 : 지도 정보와 보고자 하는 정보 간의 조화
    - 혹은 지도 정보를 단순화
    - 거리, 경로, 분포
- 관계형 : 객체와 객체 간의 관계
    - 객체는 Node, 관계는 Link
    - 크기, 색, 수
    - 휴리스틱하게 노드 배치 구성
- 계층 : 포함관계가 분명한 데이터
    - 네트워크 시각화
    - Tree, Treemap, Sunburst
- 비정형

데이터의 종류
1. 수치
    - 연속 : 길이, 무게, 온도
    - 이산 : 주사위 눈금, 사람 수
2. 범주
    - 명목 : 혈액형, 종교
    - 순서 : 학년, 별점, 등급

## 시각화 이해

마크 : 점, 선, 면
- ![[제목 없음.png]]
채널 : 마크를 변경할 수 있는 요소
- ![[제목 없음 1.png]]

전주의적 속성

- 주의를 주지 않아도 어그로 끌리는 것.
- 적절하게 사용할 때 시각적 분리가 일어남
    - 검은색 사이의 빨간색
    - 흰색 사이의 검은색
- ![[제목 없음 2.png]]

# matplotlib

라이브러리 불러오기 및 버전 확인
` import numpy as np`
`import matplotlib.pyplot as plt`
`print(np.__version__)`
`print(matplotlib.__version__)`

Figure
- 큰 틀 생성

figsize
- figure의 크기 설정

show
- 시각화
`fig = plt.figure()
`plt.show()`


`plt.add_subplot()`
- 서브플롯 생성
- 여러개 지정가능
    - fig.add_subplot(121,122)

`plt.plot`
- 데이터를 ax에 그리기

`plt.gcf().get_axes()`
- plt를 서브플롯 객체로 받아서 사용

## plot의 요소
- ax에는 동시에 다양한 그래프 그릴 수 있음.
- 이때, 동시에 그래프를 그리면 색상이 자동 지정
- 다른 종류의 그래프를 그릴 경우 색상 지정 필요

색상지정
- color파라미터
- color 명시
    1. 한 글자
    2. 컬러 이름
    3. hex code : 가장 추천

텍스트
- 정보를 추가
- label파라미터 사용
- `ax.legend()`를 해주어야 명시됨

제목
- `ax.set_title('여기에 제목입력')`
- `fig.suptitle()` : 여러 개의 그래프가 있을 때 최상단에 제목

.set_{}()

- ax에서 특정 데이터를 변경하는 경우
- 해당 정보를 받아올 때 .get_{}()

축
- ticks : 축에 적히는 수
- ticklabels : 축에 적히는 텍스트

텍스트 추가하기

1. text : 원하는 위치에 text를 적음
    - `ax.text(위치, s='여기에 텍스트 입력')`
2. annotate : 원하는 위치에 text를 저장
    - `ax.annotate(text='여기에 텍스트 입력', xy=(좌표))`
    - 화살표 추가 가능 : `ax.annotate(text,xy,xytext=(화살표좌표), arrowprops=dict(facecolor='화살표색깔'))`