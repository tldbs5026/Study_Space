
# 1. 오늘 마무리 지어야 할 일
- 심화1 코드 작성정도만?
- 


# 2. 내일 해야할 일
- 전체적인 구조 다시보기 + 심화 2코드작성 하기

# 3. 오늘 들은 강의 정리
## KL Divergence
- ![[제목 없음.png]]
- $p_\theta$ : 학습하고자 하는 데이터
- $p_{data}$ : 생성되는 일련의 데이터
- KLD는 두 확률분포 사이의 거리를 나타냄.
	- 거리라는 개념으로 생각하는 것이 이해가 더 쉽기 때문에 거리라는 단어를 사용
	- 실제로는 두 확률분포는 symmetric property를 만족하지 않기 때문에 각각의 거리는 서로 다르게 계산됨
- 목표는 KLD를 최소화 시키는 것.
	![[제목 없음 1.png]]

## ELBO




# 4. 회고록
- 복습을 하고자했지만, 오전에 다른 모델들을 이해하느라 체력을 다 소진한 것같다. 오늘보았던 모델들은 어느정도 이해가 됐다고 생각하기에, 천천히 다음 진도를 나가기 위해 다소 빠르게 하던 일을 마무리 하고자한다. 무리해서 번아웃이 빠르게 오지않도록 관리를 잘하는 것도 내게 주어진 일이라고 생각한다. 다음에는 복습을 어느정도 정리하고 코딩테스트 문제를 쉬운 것부터 한두어개정도 풀어서 정리하도록 하겠다.