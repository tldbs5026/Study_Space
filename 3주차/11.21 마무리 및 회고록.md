
# 1. 오늘 마무리 지어야 할 일
- LSTM 구조를 코드와 함께 파악
	- input, output이 어떻게 구성되어있는지

# 2. 내일 해야할 일
- 8~9
- visualization 천천히


# 3. 오늘 들은 강의 정리
LSTM 코드 구조 파악

LSTM 코드 구조 파악

![[Pasted image 20231122143119.png]]

[파이토치 문서 참고](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM)
[RNN강의](https://colab.research.google.com/drive/1pLi_4IEdG8kg_kufn-b3Pcj6PxRjE_Sj#scrollTo=fXbdiXiZPYIN)
[코드참고](https://colab.research.google.com/drive/1dPq0TdMKOJ5Z7mUFYvGMYJdAwUcjiXTh)

`class RecurrentNeuralNetworkClass(nn.Module):
 ``   def __init__(self,name='rnn',xdim=28,hdim=256,ydim=10,n_layer=3):
 ``       super(RecurrentNeuralNetworkClass,self).__init__()
 ``      self.name = name
 ``       self.xdim = xdim
 ``       self.hdim = hdim
 ``       self.ydim = ydim
 ``       self.n_layer = n_layer 
       `self.rnn=nn.LSTM(input_size=self.xdim,hidden_size=self.hdim,num_layers=self.n_layer,batch_first=True)
       `self.lin = nn.Linear(self.hdim,self.ydim)`

nn.LSTM은 LSTM모델을 생성하고, 구조를 생성

- $f_t$​=σ($W_{if}$$​x_t​$+$b_{if}$​+$W_{hf}​$$h_{t−1}$​+$b_{hf}$​)
	- forget gate
- $i_t$​=σ($W_{ii}$$​x_t$​+$b_{ii}$​+$W_{hi}$​$h_{t−1​}$+$b_{hi}$​)
	- input gate
	- 어떤 정보를 처리할지
- $g_t$​=tanh($W_{ig}$​$x_t$​+$b_{ig}$​+$W_{hg}$$​h_{t−1}$​+$b_{hg}$​)
	- cell update
	- 정보의 크기
- $c_t$​=$f_t$​⊙$c_{t-1}$​+$i_t​$⊙$g_t$
	- time t에 대한 cell state
	- ⊙ : the [Hadamard product](https://en.wikipedia.org/wiki/Hadamard_product_(matrices))
	
- $o_t$​=σ($W_{io}$​$x_t$​+$b_{io}$​+$W_{ho}​$$h_{t−1}​$+$b_{ho}$​)
	- output gate
- $h_t$​=$o_t$​⊙tanh($c_t$​)​
	- 입력 : output gate
	- 출력 : cell state $c_t$


실제 값을 넣고 hiddenstate 의 값을 반환하는 것은 rnn(input, (h0,c0))
```
    def forward(self,x):
        # Set initial hidden and cell states
        h0 = torch.zeros(self.n_layer, x.size(0), self.hdim).to(device)
        print(x.size(0))
        print(x.size(1))
        c0 = torch.zeros(self.n_layer, x.size(0), self.hdim).to(device)

        
        # RNN
        # 위에서 세팅한 lstm모델의 결과는 밑의 코드로 output을 뽑음.
      
		rnn_out,(hn,cn) = self.rnn(x, (h0,c0))
        # x:[N x L x Q] => rnn_out:[N x L x D]
        # Linear
        out = self.lin(rnn_out[:,-1:]).view([-1,self.ydim])
        return out

```
Output : output, (h_n, c_n)
- output
	- 실습의 코드는 배치가 없기 때문에 (L,D∗Hout), 있으면 (L,N,D∗Hout)
	- sequence의 마지막 h_out값 반환
- h_n
	- 배치가 없을 때 (D∗num_layers,Hout​), 있으면 (D∗num_layers,N,Hout​)
	- final forward for each element in the sequence.
	- 결과에 출력되는 부분.
- c_n
	- 배치가 없을 때  (D∗num_layers,Hcell), 있으면 (D∗num_layers,N,Hcell)
	- final cell state for each element in the sequence.
	- cell state는 결과에 출력되지 않음.
torch 문서에서 

N = batch size
L = seq_len
D = 2(bidirectional), 1
$H_in$ = input_size
$h_{cell}$ = hidden_size
$H_{out}$ = proj_size if proj_size > 0 otherwise hidden_size


gpt에게 맡긴 latex


$$
\begin{align*}
i_t &= \sigma(W_{ii}x_t + b_{ii} + W_{hi}h_{t-1} + b_{hi}) \\
f_t &= \sigma(W_{if}x_t + b_{if} + W_{hf}h_{t-1} + b_{hf}) \\
g_t &= \tanh(W_{ig}x_t + b_{ig} + W_{hg}h_{t-1} + b_{hg}) \\
o_t &= \sigma(W_{io}x_t + b_{io} + W_{ho}h_{t-1} + b_{ho}) \\
c_t &= f_t \odot c_{t-1} + i_t \odot g_t \\
h_t &= o_t \odot \tanh(c_t)
\end{align*}
$$
- 간단 작업은 gpt에게 맡깁시다.


[파이토치 문서 참고](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM)
[RNN강의](https://colab.research.google.com/drive/1pLi_4IEdG8kg_kufn-b3Pcj6PxRjE_Sj#scrollTo=fXbdiXiZPYIN)
[코드참고](https://colab.research.google.com/drive/1dPq0TdMKOJ5Z7mUFYvGMYJdAwUcjiXTh)

`class RecurrentNeuralNetworkClass(nn.Module):
 ``   def __init__(self,name='rnn',xdim=28,hdim=256,ydim=10,n_layer=3):
 ``       super(RecurrentNeuralNetworkClass,self).__init__()
 ``      self.name = name
 ``       self.xdim = xdim
 ``       self.hdim = hdim
 ``       self.ydim = ydim
 ``       self.n_layer = n_layer 
       `self.rnn=nn.LSTM(input_size=self.xdim,hidden_size=self.hdim,num_layers=self.n_layer,batch_first=True)
       `self.lin = nn.Linear(self.hdim,self.ydim)`

nn.LSTM은 LSTM모델을 생성하고, 구조를 생성

- $f_t$​=σ($W_{if}$$​x_t​$+$b_{if}$​+$W_{hf}​$$h_{t−1}$​+$b_{hf}$​)
	- forget gate
- $i_t$​=σ($W_{ii}$$​x_t$​+$b_{ii}$​+$W_{hi}$​$h_{t−1​}$+$b_{hi}$​)
	- input gate
	- 어떤 정보를 처리할지
- $g_t$​=tanh($W_{ig}$​$x_t$​+$b_{ig}$​+$W_{hg}$$​h_{t−1}$​+$b_{hg}$​)
	- cell update
	- 정보의 크기
- $c_t$​=$f_t$​⊙$c_{t-1}$​+$i_t​$⊙$g_t$
	- time t에 대한 cell state
	- ⊙ : the [Hadamard product](https://en.wikipedia.org/wiki/Hadamard_product_(matrices))
	
- $o_t$​=σ($W_{io}$​$x_t$​+$b_{io}$​+$W_{ho}​$$h_{t−1}​$+$b_{ho}$​)
	- output gate
- $h_t$​=$o_t$​⊙tanh($c_t$​)​
	- 입력 : output gate
	- 출력 : cell state $c_t$


실제 값을 넣고 hiddenstate 의 값을 반환하는 것은 rnn(input, (h0,c0))
```
    def forward(self,x):
        # Set initial hidden and cell states
        h0 = torch.zeros(self.n_layer, x.size(0), self.hdim).to(device)
        print(x.size(0))
        print(x.size(1))
        c0 = torch.zeros(self.n_layer, x.size(0), self.hdim).to(device)

        
        # RNN
        # 위에서 세팅한 lstm모델의 결과는 밑의 코드로 output을 뽑음.
      
		rnn_out,(hn,cn) = self.rnn(x, (h0,c0))
        # x:[N x L x Q] => rnn_out:[N x L x D]
        # Linear
        out = self.lin(rnn_out[:,-1:]).view([-1,self.ydim])
        return out

```
Output : output, (h_n, c_n)
- output
	- 실습의 코드는 배치가 없기 때문에 (L,D∗Hout), 있으면 (L,N,D∗Hout)
	- sequence의 마지막 h_out값 반환
- h_n
	- 배치가 없을 때 (D∗num_layers,Hout​), 있으면 (D∗num_layers,N,Hout​)
	- final forward for each element in the sequence.
	- 결과에 출력되는 부분.
- c_n
	- 배치가 없을 때  (D∗num_layers,Hcell), 있으면 (D∗num_layers,N,Hcell)
	- final cell state for each element in the sequence.
	- cell state는 결과에 출력되지 않음.
torch 문서에서 

N = batch size
L = seq_len
D = 2(bidirectional), 1
$H_in$ = input_size
$h_{cell}$ = hidden_size
$H_{out}$ = proj_size if proj_size > 0 otherwise hidden_size


gpt에게 맡긴 latex


$$
\begin{align*}
i_t &= \sigma(W_{ii}x_t + b_{ii} + W_{hi}h_{t-1} + b_{hi}) \\
f_t &= \sigma(W_{if}x_t + b_{if} + W_{hf}h_{t-1} + b_{hf}) \\
g_t &= \tanh(W_{ig}x_t + b_{ig} + W_{hg}h_{t-1} + b_{hg}) \\
o_t &= \sigma(W_{io}x_t + b_{io} + W_{ho}h_{t-1} + b_{ho}) \\
c_t &= f_t \odot c_{t-1} + i_t \odot g_t \\
h_t &= o_t \odot \tanh(c_t)
\end{align*}
$$
- 간단 작업은 gpt에게 맡깁시다.

# 4. 회고록
- 모델을 구현한 코드를 보다보니 단순히 눈으로 익히는 것에는 한계가 있음을 느껴서 직접 파이토치 document를 찾아보며 각 코드의 구성을 확인했다. 직접 코드로 하나하나 짜보는 것도 도움이 되지만, 이러한 구조는 먼저 코드를 어떻게 이론상으로 구현했는가를 확인한 후 추후 하나씩 쌓아올려가보는 경험을 하는 것이 더 도움이 된다고 생각했다. 따라서 순서를 먼저 코드를 쳐보고 이해하는 것이 아닌, 먼저 모델의 component에 대한 이해를 한 후 작성해보고자 한다.