## KL Divergence
- ![[제목 없음.png]]
- $p_\theta$ : 학습하고자 하는 데이터
- $p_{data}$ : 생성되는 일련의 데이터
- KLD는 두 확률분포 사이의 거리를 나타냄.
	- 거리라는 개념으로 생각하는 것이 이해가 더 쉽기 때문에 거리라는 단어를 사용
	- 실제로는 두 확률분포는 symmetric property를 만족하지 않기 때문에 각각의 거리는 서로 다르게 계산됨
- 목표는 KLD를 최소화 시키는 것.
	![[제목 없음 1.png]]
