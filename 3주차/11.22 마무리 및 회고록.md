
# 1. 오늘 마무리 지어야 할 일
- 모더레이터 발표 재료 찾기
- positional encoding 정리


# 2. 내일 해야할 일
- 강의 나머지 + 과제 5마무리 + 심화 손대기


# 3. 오늘 들은 강의 정리
## Positional Encoding
- 위치 정보를 어떻게 넣는가?
- 각 단어의 상대적인 위치 정보를 네트워크에게 입력하기 위한 주기 함수 이용
	- 왜 주기함수인가?
	- seq2seq에서도 positional encoding이 쓰이긴 했지만, 단어의 임베딩에 위치 정보를 더했음.
	- [transformer](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)
		- 모델이 relative 한 position에 대해 더 잘 학습할 것이라 가정하였고, 고정된 offset(주어진 위치에서의 거리)k가 주어질때 이를 선형으로 설명할 수 있게 될 것이라 생각하였음.
		- 실험해보니까 더 멀리 예측 잘하더라.
		- We also experimented with using learned positional embeddings [8] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training(Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. _Advances in neural information processing systems_, _30_.)
- [참고](https://www.youtube.com/watch?v=AA621UofTUA)
	- ![[Pasted image 20231122114950.png]]
	- pos : 각 단어의 번호(idx)
	- i : 각 단어에 대한 임베딩 값의 위치
	- d_model : 임베딩 차원
	- 다른 주기 함수도 사용 가능
	- 일반 임베딩 값과 위치 인코딩을 element-wise 덧셈
		- ![[Pasted image 20231122115306.png]]

# 4. 회고록
- 코딩테스트 다시 재활훈련 해야함.
- 이해할 수 있는 부분은 이해하고, 아닌부분은 주말에 천천히 이해해봅시다.