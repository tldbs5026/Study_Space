## Positional Encoding
- 위치 정보를 어떻게 넣는가?
- 각 단어의 상대적인 위치 정보를 네트워크에게 입력하기 위한 주기 함수 이용
	- 왜 주기함수인가?
	- seq2seq에서도 positional encoding이 쓰이긴 했지만, 단어의 임베딩에 위치 정보를 더했음.
	- [transformer](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)
		- 모델이 relative 한 position에 대해 더 잘 학습할 것이라 가정하였고, 고정된 offset(주어진 위치에서의 거리)k가 주어질때 이를 선형으로 설명할 수 있게 될 것이라 생각하였음.
		- 실험해보니까 더 멀리 예측 잘하더라.
		- We also experimented with using learned positional embeddings [8] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training(Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. _Advances in neural information processing systems_, _30_.)
- [참고](https://www.youtube.com/watch?v=AA621UofTUA)
	- ![[Pasted image 20231122114950.png]]
	- pos : 각 단어의 번호(idx)
	- i : 각 단어에 대한 임베딩 값의 위치
	- d_model : 임베딩 차원
	- 다른 주기 함수도 사용 가능
	- 일반 임베딩 값과 위치 인코딩을 element-wise 덧셈
		- ![[Pasted image 20231123145449.png]]
		- 해주는 이유? 주기함수를 사용함으로써 일정한 크기를 유지하여 vanishing or explode의 가능성을 배제함과 동시에 vector 공간 상에서 임의의 위치로 해당 임베딩 vector를 지정시킴
