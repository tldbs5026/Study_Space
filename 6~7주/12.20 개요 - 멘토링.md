# 데일리 스크럼


- deepCoNN
	- 임베딩 차원 늘렸는데 학습이 느려짐
	- baseline에서 feature emb를 64로 늘리는 것으로 확정

- 2.175

- RotCNN
	- valid loss가 2.3이하로는 안내려감
	- 정리하고 앙상블 예정


readme.md
- 여기에 적기?

issue대신 readme에
- 관리의 과거내역이기 때문에
- 면접관은 readme를 더 볼것이다?

wrapup report에 더 시선이 가서..
- readme가 더 관리가 편할것.

readme.md정리에 초점

## 내용 바꾼부분


- main - args추가
- image_data 
- CNN_FM_model 코드 변형



- 스크립트 파일 추가
- 파일 변형은 없었음.
- main - wandb


- DCN 수정


- text 파일.


- DeepCoNN
- text data

시윤
- context_data


- 베이스 + 수정 : 모델,데이터처리
- ensemble,main,은 그대로, 건드린  모델만 추가해서 


# 멘토링

## Feature Selection
- 전처리함에 있어 정확도를 높일 수 있는 방법

상관을 통해 선택한 후, Wrapper를 통해 추가적으로 선택,제거


Filter
- 상관
	- 변수 간의 관계를 시각화해서 제거
- 분산분석
	- 각 그룹별 통계를 통해 차이를 확인
Wrapper
- 실제 모델의 성능을 활용하여 변수를 선택
- 모델을 반복적 학습, 검증하는 과정
- Forward
	- Feature가 없는 상태에서 Feature를 추가
- Backward
	- 모든 Feature를 기준으로 시작해 Feature 제거
	- 하기쉬움, 자주쓰임
Embedded
- Feature Importance를 이용
	- 노드의 순수도를 나타내는 Gini, Entropy를 활용
- 트리모델 기반
	- 트리 노드 분할에 대한 기여도를 이용해 Feature 선택
- 규제기반


적대적 검증_Adversarial Validation
- overfitting
- 학습은 0, 검증 데이터셋은 1
- 학습 + 검증의 데이터셋을 무작위로 분할
- 모델 결과를 AUC, Feature Importance 해석

Permutatino importance
- Target Permutation
- 학습 모델이 구성된 후 Feature의 중요도 파악
- Feature를 하나 선택해 무작위로 섞은 후 학습된 모델 예측
	- Feature를 섞을 때 점수가 떨어진 경우는 중요한 feature
- 재학습 할 필요가 없음
- sklearn.inspection import permutation_importance


문제정의
- 이미지 - 유저들의 interaction + 책의 메타정보
	- 정보성이 충분히 유효한가?

옷 추천/개인화 추천 -> 옷 정형화 (상의,하의)
	북 커버 : 출판사에 의해 만들어진 임의의 feature
	crop, argumentation


실무에서는?
- 결측치가 많을수밖에 없음
- 일반적으로는 domain에 따라서
	- 추측(여성일 것이다, 노인일 것이다..)

gender, age -> 이름을 기반으로 추론..

## 앙상블 
트리 계열이 강력함.
 