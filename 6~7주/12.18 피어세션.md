
어디까지 공부했는가
재권
- Deep_CoNN
	- baseline
	- text_summary만 사용
	- users의 임베딩에서는 유저가 선호하는 book_summary를 합친 형태.
	- 결측치 40%
		- None

- 책 제목을 대신 
- 이미 summary가 있는 경우에도 제목을 붙여서
- 
bert
- segment

재원
- Deep_CoNN
- summary가 빈약
- 결측 + 내용의 퀄리티
- users에서 summary데이터에서 뽑을 때, book의 summary에서 가져옴.
	- book title을 None값에 넣고, summary가 있어도 book_title을 집어넣음
	- 논문참고, 파라미터 세팅, padding - 과적합
	- 대부분의 성능이 좋지 않음을 확인.
- gradient boostring위주로

텍스트보다는 메타데이터가 더 성능이 좋음.
그것을 잘 활용하는게 ML모델


훈
- Gradient Boosting의 과적합 해결을 위해 DL과 앙상블

- 가정
	- 1. boost : 개인의 특징
	- 2. dl : 전체적인 feature
 boosting기반
 - catboost
 - hyperparametetr tuning
	 - grid search과 같은.
 - n-fold validation

book rating
- 커뮤니티 데이터의 특성에 대한 생각

GCN
- 메타 데이터, 이미지 등
- 코드를 많이 만들어야함.

현주
- args param을 받아 cnn에 받도록.
- 내일 확인하고 깃헙에 올릴듯.
- 결측을 빼는 것이 더 성능이 안좋아서 포함해서 진행할 것으로 예상.
	- 결측치에서 얻을 수 있는 특성
	- 1로 채워짐 - 점으로 만들어짐.


random_forest, xgboost가 더 성능이 좋은 경우가 많음.
- 물론 데이터에 따라.



