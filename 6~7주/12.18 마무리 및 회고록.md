

# 오늘까지 마무리 할 일
- Deep CoNN 모델 돌려보기
- 

# 내일 할 일
- 깃허브에 등록

# 공부하면서 내용 정리
bert model의 구조_코드

```
for sent in tokenize.sent_tokenize(text):  # 입력된 텍스트를 문장으로 분리

        text_ = "[CLS] " + sent + " [SEP]"  # 각 문자 앞에 CLS 토큰, SEP 토큰 추가. - 각 문장의 시작과 끝을 알림.

        tokenized = tokenizer.tokenize(text_)  # 추가한 토큰을 갖는 문장을 토큰화

        indexed = tokenizer.convert_tokens_to_ids(tokenized)  # 토큰화된 문장을 bert 모델이 이해할 수 있는 idx로 변환

        segments_idx = [1] * len(tokenized)    # 각 토큰에 대한 세그먼트 인덱스 생성.

        token_tensor = torch.tensor([indexed])  # 텐서 변환

        sgments_tensor = torch.tensor([segments_idx])  # 세그먼트 인덱스 변환

        with torch.no_grad():  # 모델 실행

            outputs = model(token_tensor.to(device), sgments_tensor.to(device)) # 각 token, sgments를 입력으로 받아 결과를 처리 한 후 나오는 output 반환

            encode_layers = outputs[0]  # 모델의 첫 번째 출력 추출. == 인코딩 레이어의 출력

            sentence_embedding = torch.mean(encode_layers[0], dim=0)  # 첫 번째 문장들을 받아, 그 평균 계산한 문장 임베딩

    return sentence_embedding.cpu().detach().numpy()  # cpu로 바꾸고 detach로 해제, numpy로 변환
```

# 회고록

프로젝트 종료까지 4일 남았다. 2주 동안 몇 가지 전에 해보지 못했던 시도들을 해보았던 것 같다. 전처리를 어떻게 할지, 전에 사용해볼까 생각했던 모델들의 구조를 파악하고, 어떻게 구동되는지 확인해보고, 성능을 어떻게 높일 수 있을지 고민해보는 길지만 짧은 시간이라고 생각한다. 몇 개는 생각한대로 구동되었고, 몇 개는 아직 시도중이고, 결과를 예측했을 때 좋지 않은 결과를 보이는 작업도 있었지만, 모든 것들이 고민했던 결과들이라고 생각한다. 또한 이런 경험들은 잊혀지지 않고 계속 쌓일 것이고, 더 나은 결과를 보여줄 것이라 믿는다. 또한, 이론으로 생각했던 것들이 실제로 어떻게 작용하는지를 확인해 볼 수 있는 기회가 되었다고 생각한다. 남은 기간, 최대한 열심히 하지만 쓰러지지 않을 정도로 하여 최대한 내가 생각했던 것들을 다 구현해보고 싶다.