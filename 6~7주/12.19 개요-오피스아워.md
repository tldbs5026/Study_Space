# 데일리 스크럼
재권
- DeepCoNN 임베딩 차원 증가
	- 유저-아이템 차원 32 to 64
	- epoch는 valid loss는 증가, 실제 점수는 증가.
	- 결측치 타이틀 대체, 유의미하지는 않았음.
	- 그래도 결측치를 채우는 것이 낫다.
- Bert 기본모델 대신 Decoding Enhanced Bert
- 전처리만 손대고 마무리


시윤
- 베이스라인 다른거 손대기.
- WDN



재원
- RopCNN
	- 재현
승아
- epoch 증가
- NCF
훈
- 병합작업 마무리


현주
- CNN-FM에서 


시윤
- WDN


깃헙 병합?
- 데이터 전처리 파일만 묶으면
- 일단 좀 더 생각해보기.

# [오피스아워](file:///C:/Users/kimsiyun/Downloads/[%EC%98%A4%ED%94%BC%EC%8A%A4%EC%95%84%EC%9B%8C]%20RecSys%EA%B8%B0%EC%B4%88_%ED%97%88%ED%98%84_231214%20(%EB%B0%9C%ED%91%9C%EC%9E%90%EB%A3%8C).pdf) - Online Test
- 인과추론, 데이터 관리를 통한 서비스 성장


온라인 테스트를 하는 이유
- 오프라인 != 실제 성능

## 오프라인의 성능이 실제 개선으로 이어지지 않는 이유
데이터
- 과거 데이터의 결과에 편향될 가능성
- 추천 시점에서의 편향
- concept drift로 과거와 현재의 서비스 행태의 변경
- 오프라인의 피쳐가 실제로 크게 유의하지 않는 경우

지표 관점
- 작은 지표는 실제로는 유의하지 않은정도
- 통계적인 유의성
- 온라인과 오프라인에서의 지표의 중요성 차이
	- 온라인 : 클릭,구매,구매금액..
	- 오프라인 : rmse...

상관성
- 상관 != 인과


인과성 증명
- 동일한 대상에 개입, 처치 진행 후 처치한 대상과 아닌 대상 간의 차이 확인
- 사회실험에서는 불가능
	- 두 대상이 다르기 때문
	- 처치를 한 것과 아닌것 둘 중 하나밖에 못구함

- RCT
	- 많은 유저를 확보하고 랜덤하게 그룹을 나눔
		- 무작위 sampling
		- 큰 수의 법칙
		- 온라인에서 A/B라고 불림

### 실무에서의 테스트

대상 선정 == 트리거
- 특정 트리거를 달성한 사람 기준


랜덤화 단위 선정
- 트래픽 단위로 랜더마이즈 유닛 선정
- 방법
	- 짝, 홀
		- 비율, 그룹의 수..
	- 매번 random 함수
		- 로직 통제 불가
	- user id등을 이용해 미리 그룹 할당
		- 실험 그룹, 버킷
		- 주로 선호됨
	- salt를 이용한 해시 함수를 이용한 할당
		- ![[Pasted image 20231219172810.png]]

### 결과 분석
성공 지표
- 클릭율
- 체류 시간
- 구매율..

가드레일 지표
- 훼손되면 실험 중단
	- 앱 삭제
	- 컨텐츠 신고..

통계적 유의성
- 빈도주의, 베이지안
- 빈도주의 : 편의성
- [빈도주의 Optimizely서비스 결과 표시](https://www.mugo.ca/Blog/How-to-use-Optimizely-to-create-your-first-A-B-Testing-experiment)
- [베이지안 googleoptimize서비스 결과 표시](https://kinsta.com/blog/google-optimize/)

### 실험 결과를 대하는 태도
- 대부분은 실패할 가능성이 높음
- 좌절보다는 개선점에 대한 인사이트
- 평균 처치효과 + 세그먼트 필터링한 조건부 평균 처치효과 확인
- 하이브리드 필터링


## 실무적 고려사항


AA테스트
- 랜덤하게 나누어지는지 확인
- AB테스트 전 같은 A를 두 그룹으로 나누어
	- 통계적으로 유의하게 차이가 있는지 확인

Sample Ratio Mismatch
- 50 50 할당 실험이지만 30 70일 경우 데이터 신뢰 불가
	- SRM이 발생했다 이야기
	- 할당 로직 버그, 지연시간 차이, 결측치 등
- 추천시스템의 로딩시간도 이에 해당됨

Counterfactual Logging
- 다른 그룹에서의 실험 로깅
- Counterfactual(반사실)은 실제 발생하지 않은 사실을 의미
- 추천시스템에서 A유저가 control그룹에 할당되었다면 control 추천시스템으로 추천된 결과가 사실,variant 추천시스템으로 추천된 결과가 반사실
- CounterfactualLogging이란 상대 그룹의 추천 시스템으로 추천되었을 결과까지 로깅하는 것을 의미
- 두 추천 시스템의 추천이 같다면 실험 결과 해석에 사용할 결과 데이터에서 제외하여
보다 뾰족하게 통계적 유의성 확인 가능
- 두 추천 시스템 로딩 시간이 차이난다면 늘어난 로딩 시간으로 사용자 경험을 헤치지 않게 설계 필요

## 추천 도서
- TrustworthyOnlineControlledExperiments
(A/B테스트 - 신뢰도 높은 온라인 통제 실험)



## 커리어의 작업 변화

NC
- 데이터 분석에서 정해진 업무
- 유저의 세그먼트, 작업장 걸러내는 로직의 고도화

에듀테크
- 특정 그룹에서 여러 직무가 모인 태스크포스
	- 데이터 도메인과 관련
	- 데이터 플로우부터, 알고리즘, 로직 개선, 지표 시스템화 등

데이터의 중요성
- 중요한 지표의 파악
	- 정리 및 공유하는 노력
- 데이터의 흐름

Counterfactual Logging
- a라는 추천시스템 : control
	- 일반적으로는 a 에 대한 로깅
- b라는 추천시스템 : test
	- 노출하지는 않지만, 로깅으로만 남는
	- 어쨌든 돌리긴함. 근데 뚜렷하게 나타나지는 않음.

온라인 환경 구축/ 이에 대한 분석 BI 설계
- 외부 SAS 서비스
	- 환경 구축 + 리포팅까지는 해줌
- 내부 구축 서비스
	- 더 고도화된 니즈, 신뢰성에 의해서 + 비용
	- 실험 할당 구축 / 환경 구축 / 분석
	- 분석은 환경이 구축된 후 후속적인 조치로 파악

온라인 테스트 모델 선정
- 오프라인의 결과를 통해 어느정도 성능 예측한 후 모델 선정
- 실험의 결과로 선택
- 추가적인 테스트는 가설을 세운 후 로직대로 진행


모델 업데이트 과정
- 실제 돌아가는 모델에서는 계속 반영이 됨
- 온라인은 세부 파라미터를 계속 건드려서 수정.
- concept drift는 컨설팅하는 중에는 어느정도 해소가 됨.
	- 자료의 최신성에 맞추어 수정하기 때문.


취업 - 각 경험에서 어필해야 할 부분 
- 프로세스, 스킬 ...
- 어려움을 해결하는 과정을 설명할 수 있는가?
	- 이 과정에서 협업, 강화된 스킬을 어필
- 대회에서는 반복적으로 바꾸는 것이 점수를 높이는 방법인데, 현업에서는 가설에 기반
	- 가설을 세우고, 이에 맞춰 모델을 바꾸고 모델을 변경, 개선, 확인,,,,
	- 이 iteration을 얼마나 잘했고, 과정에서 얼마나 많은 고민을 했는가
	- 문제해결의 포인트

현업에서의 어려움을 느끼는 지점
- 타임라인
- QA

각 해결상황에서 키워드
- 모델을 돌렸을 때의 결과, 피어세션에서의 대화, 고민하는 부분, 고민을 어떻게 개선했는지
- 이런 내용을 기록하는 것이 중요.


