

# 오늘까지 마무리 할 일
- 과제 3_ gpu초과로 내일 하기


# 내일 할 일
- 전체 마무리 빨리 짓고 심화까지 같이
- SQL관련 강의 듣기
- 코드 다시 보면서 정리하기
- 구인구직

# 공부하면서 내용 정리

## 과제3

torch.sum(dim=(2,1))
- 특정 차원에서 합을 계산하는 것.
```
tensor = torch.tensor([[[1, 2, 3], [4, 5, 6]], 
						[[7, 8, 9], [10, 11, 12]], 
						[[13, 14, 15], [16, 17, 18]]]) 

result = torch.sum(tensor, dim=(2, 1))
```
주어질 때 이는 열(2)을 기준으로 행(1)에 해당하는 모든 elements을 합하라. 
```
result: [21, 57, 93]
```

```
input_dim = fm_df.shape[1] - 1
batch_size = 256
data_shuffle = True
task = 'clf'
factorization_dim = 8
epochs = 3  # 20
learning_rate = 0.001
gpu_idx = 0
```

FM의 공식
$$\mathrm{FMLayer}\left( \mathrm{x} \right) = \frac{1}{2} \sum_{f=1}^{k} \left[ \left( \sum_{i=1}^{n} v_{i,f} x_i \right)^2 - \sum_{i=1}^{n} v_{i,f}^2 x_i^2 \right]$$

전항 : sum of square
- v : 가중치 행렬 (input_dim, factor_dim)
- x : input  (batch_size, input_dim)

후항 :  square of sum
- v : 가중치 행렬 (input,factor)
- x : input (batch, input)

결과는 (batch, factor) -> sum(dim=1) -> input을 기준으로 batch의 합 --> (batch,) 
$$\hat{y}(\mathrm{x}) = w_0 + \sum_{i=1}^{n}{\mathrm{w}_i x_i} + \mathrm{FMLayer}\left( \mathrm{x} \right) \\ \, = \mathrm{Linear}\left( \mathrm{x} \right) + \mathrm{FMLayer}\left( \mathrm{x} \right)$$
nn.Linear(input, output, bias=~)
- 받은 input을 1차원 output으로 반환
- 집어넣어주는 input은 대부분 (batch, input_dim)으로 구성
- 여기서 Linear는 batch당 output_dim으로 input_dim을 압축

```
self.linear = nn.Linear(input_dim, 1, bias=True)
```
- x(batch_size, input_dim)의 값은 하나의 element로 반환되어야함. -> input_dim을 1차원으로 반환해야하는 것.
	- $\sum$ = batch_size
- 이 때 linear의 결과는 (batch_size,1)로 반환
- 이전의 fm(batch,)이기 때문에 squeeze를 해주어 (batch,) 1차원으로 계산


## FFM

$$ \mathrm{FFMLayer} \left( \mathrm{x} \right) = \sum_{i=1}^{n} \sum_{j=i+1}^{n}{\left \langle \mathrm{v}_{i,f_j} \, , \mathrm{v}_{j,f_i} \right \rangle x_i x_j} \\ \qquad \qquad \quad = \sum_{i=1}^{n} \sum_{j=i+1}^{n}{\left \langle x_i \mathrm{v}_{i,f_j} \, , x_j \mathrm{v}_{j,f_i} \right \rangle}$$
x : (batch,input_dim)
v : 학습 시켜야하는 emb의 차원 (input, field_dim)
`nn.Embedding(feature_size, factor_dim) for feature_size in field_dims])
	- list comprehension : field의 개수를 의미


$$\hat{y}(\mathrm{x}) = w_0 + \sum_{i=1}^{n}{\mathrm{w}_i x_i} + \mathrm{FFMLayer}\left( \mathrm{x} \right) \\ \, = \mathrm{Linear}\left( \mathrm{x} \right) + \mathrm{FFMLayer}\left( \mathrm{x} \right)$$

- x : (batch, num_field)
- x_multihot : multi hot encoding (batch, input_dim)
	- multihot : 여러 범주에 대해 여러 차원
	- onehot : 각 범주, 하나의 차원
	- scatter : 텐서의 특정 위치에 값을 할당 (dim_index, 값이 할당될 위치의 idx, 할당될 값 )
		- input_dim에 존재하는 x를 1로 할당, 나머지는 0
- FFMLayer (batch, )
- Linear(x) : (batch,)

- y =  (self.linear(x_multihot).squezee() + self.ffm(x)) (batch, 1) $\rightarrow$  (batch,)+ (batch,)

Bandit
- 팔, 각각의 확률에서의 reward
$$q_*(a)  \triangleq \mathbb{E}
[R_t | A_t = a]$$
- 어느 시점 t에서의 액션이 a라는 reward를 얻는 실제 기대값.

# # 심화2
### 라플라시안 행렬

L :   D-A

D : 차수 행렬로 각 차수에 해당하는 그래프가 얼마나 연결했는가
A : 연결 =1, 나머지 = 0 
![[Pasted image 20231207111256.png]]


```
coo = X.tocoo().astype(np.float32)
        i = torch.LongTensor(np.mat([coo.row, coo.col]))    # 비어 있지 않은 좌표
        v = torch.FloatTensor(coo.data)    # 그 좌표의 value
        res = torch.sparse.FloatTensor(i, v, coo.shape).to(device)    # i의 좌표에 v값을 갖는, coo.shape의 형태를 갖는 FloatTensor
```
tocoo : to_Coordinate
- sparse tensor coo포맷
- 희소 행렬을 저장하는 format 방식
- Scipy기반
- 데이터의 비어 있지 않은 공간의 좌표, 값 출력


Light GCN
- NGCF의 w, sigmoid가 성능저하
- user emb를 concat에서 sum으로
- 마지막에 weighted sum agg
- 빠른 연산 + sota

FFM
- field_dim * factor_dim
- 상호작용 = k차원의 factor_vector 간의 dot product
- fieldt따라 서로 다른 field간의 상호작용
- 학습시 Adagrad
- Pairwise interaction tensor Factorization

Wide Deep
- wide : 단순, 해석 가능, 학습 데이터가 없는 feature에 취약, 피쳐 엔지니어링 필요
- deep : 복잡, 일반화, 학습 데이터에 있는 희소한 조합에 취약

Thompson Sampling
- MAB 문제에서 최소한의 탐색으로 각 선택지의 이익 추론
- 베타분포(어떤 결과의 확률에 대해 확신할 수 있는 정도) 활용,
- 매 스탭마다 선택 후 베타 분포 갱신
- sampling은 일정 step이후 수렴
- 매 스탭마다 샘플링할 경우 기댓값에 따라 샘플링 값은 점점 작아짐.


# 회고록
하드디스크를 옮기다가 데이터가 날아가 버린 참사가 발생하였다. 그렇다고 좌절하고 있을수만은 없다. 최대한 빨리 부캠활동과 관련한 부분만 복구하고 정상적인 생활을 다시 할 수 있도록 최대한 조치를 취해야겠다. 나중에 일을 하면 어떤 일로인해 돌이킬 수 없는 상황이 발생할 수 있다고 생각한다. 오늘을 교훈삼아 패닉하지 않고, 침착하게 일을 해결할 수 있도록 마음가짐을 굳게 먹어야겠다. 오늘 피어세션에서 각자 어떤 것을 하고 싶은지에 대한 이야기를 나누었는데, 나는 정해졌다고 생각했는데, 아직까지도 명확한 단어로 표현할 수 없었다. 좀 더 명확하게 이야기하고 싶다는 생각이 들었다.