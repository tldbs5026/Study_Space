
# 1. 오늘 마무리 지어야 할 일
- 휴식과 운동 무조건 합시다
- 


# 2. 내일 해야할 일
- 피어세션에서 받은 정보를 바탕으로 과제 제출전까지 다시 천천히 해보기
	- ![[Pasted image 20231116181218.png]]
	- !pip list | grep torch #torch관련 패키지 버젼 확인하기
		!pip uninstall torch torchdata torchtext torchvision torchaudio torchsummary #최신버전이면 다 지워줍시다
		!pip install torchvision==0.15.2+cu118 -f https://download.pytorch.org/whl/cu118/torch_stable.html
		!pip list로 버전확인 필요
	
- 실습부터 과제까지 쭉 훑어보고, 급하게 넘어간 부분이 있는지 확인할 것.
- 이 작업은 시간에 쫓기지 않고 여유롭게 해보는 것이 중요하다고 생각.


# 3. 오늘 들은 강의 정리
## num_workers
- 데이터를 불러올 때 사용하는 서브 프로세스 개수
- 내가 이해한 것은 cpu몇 개 쓸거냐 였는데
- 너무 적으면 많은 데이터를 처리하기 어렵고, 너무 많으면 gpu와의 병렬작업에서 병목현상이 일어남

## datasets
- built-in dataset을 가져올 수 있음
- torchvision.datasets.MNIST/CIFAR10()
- torchtext.datasets.
- 이러한 데이터셋을 불러온 후 데이터로더에 장착

## 텍스트
torchtext
- torchvision에서의 dataset은 불러온 후 데이터로더에 장착
- 텍스트는 tokenizing 작업을 거친 후 데이터로더에 장착

1.tokenizing
- torchtext.data.utils,get_tokenizer()
- counter를 통해 각 label에 해당하는 line을 tokenizer에 업데이트

2.vocab
- torchtext.vocab.vocab(ordered_dict, min_freq=1)
- indices된 토큰들을 매핑하여 vocab object를 생성

3.encode
- vocab.get_stoi() : 토큰을 인덱스에 매핑하는 딕셔너리로 반환
- tokenizer의 결과를 idx,value로 enumerate로 받음
4.decode
- vocab.get_itos() : 토큰을 인덱스에 매핑하는 리스트로 반환
- encode의 결과를 최종적인 리스트로 반환

4. vectorizer
- CountVectorizer()
- corpus의 리스트에 대해  fit.transform()을 통해 변환
- 그 이후 corpus를 transform하고 .toarray()로  array 변환

5. bow(bag of words)
- 각 단어가 문서에 얼마나 자주 나타나는지를 벡터로 표현
- encode : 입력된 텍스트를 숫자로 인코딩
- torch.zeros(bow_vocav_size) : BoW 벡터를 만들기 위해 초기화된 텐서
	- 각 원소는 단어의 등장 횟수
- for i in encode(text) : 입력된 텍스트를 인코딩한 결과
- res[i] +=1 : BoW벡터에서 해당 단어의 인덱스에 해당하는 원소 +=1
	- 즉, 해당 단어가 문서에 들어가있으면 추가

text의 collate_fn
- bowify
	- stack의 결과 반환
	- LongTensor의 결과 반환


# 4. 회고록
- 빠르고 정확하게 하는 것이 중요하지만 나의 상태를 아는 것도 중요하다고 생각한다. 새로운 개념을 빠르게 받아들일 수 있으면 좋겠지만, 나의 경우는 그런사람이 아니라고 생각하기에 일단 막히는 부분까지 최대한 나아가보고, 그 후에 시간을 비워서 최대한 많이 곱씹어보는 것이 더 중요하다고 생각한다. 부캠에서는 이것저것 테스트해보라고 하셨던 내용이 생각난다. 그렇기에 주어진 시간에 빠르게도 해보고, 천천히도 해보고, 복습도 계속 해보면서 나에게 맞는 스타일을 공부법을 찾아 보려고 한다. 