
# 1. 오늘 마무리 지어야 할 일
- 멘토링을 바탕으로 논문을 읽어보았다.
- 일단은 가볍게 cnn


# 2. 내일 해야할 일
- 하루를 마무리 하기 전에 어떤일을 해야하는지 적습니다.
- 내일 강의 진도
- 내일까지 제출해야하는 어떤 것
- 발표자료 준비 등등


# 3. 오늘 들은 강의 정리
AutoGrad
	- 논문은 하나의 명령을 가진 함수가 쌓인 레이어가 쌓인 모델로 구성된다.
	- 여기서 레이어(블록)을 조립하는 것이 중요하다.
	- torch.nn.Module
		- DL을 구성하는 레이어의 base class
		- 여기서 input, output, forward, backward를 정의한다.
		- 또는 학습의 대상이 되는 parameter를 정의한다.
	- nn.Parameter
		- tensor 객체를 상속
		- attribute가 될 때는(학습의 대상이 되면) requires_grad = True로 지정
		- 직접하는 것이 아닌 가져올 때는 알아서 지정되어 있다
	- backward
		- 레이어에 있는 parameter의 미분
		- forward의 값으로 나온 y_hat과 실제 label간의 차이에 대한 미분
		- 미분 결과를 바탕으로 parameter 업데이트
	
데이터셋
- ![[Pasted image 20231114210947.png]]
- 데이터 입력 형태를 정의하는 클래스
- 이미지, 텍스트 등 종류에 따라 함수를 다르게 정의.
- 정의 
	- from torch.utils.data import Dataset
	- class CustomDataset(Dataset):
		def __init__(self, text, labels):    # 초기 데이터 생성 방법
			self.labels = labels
			self.data = text
		def__len__(self):   # 데이터의 전체 길이
			return len(self.labels)
		def __getitem__(self, idx):
			label = self.labels[idx]    # idx값을 주었을 때 반환되는 데이터의 형태
			text = self.data[idx]
			sample = {"Text": text, "Class": label}
			return sample
	- 데이터 생성 시점에 바로 처리할 필요가 없다.
	- 허깅페이스와 같이 표준화된 처리방법을 제공해줘야 다른 사람이 쓰기 편하다.

- DataLoader
	- Data의 Batch를 생성
	- gpu에 feeding 전 데이터의 변환
	- 텐서 변환 + Batch 처리
	- 병렬적인 데이터의 경우는 전처리를 추가적으로 해야함
	- 기본적으로 generator의 형식이기 때문에 next(iter(MyDataLoader))으로 데이터를 로딩해줘야함
	- collate_fn
		- [Data,Label]의 형태로 묶인 데이터를 [Data,Data], [Label,Label]로 묶어줌 
		- 각각 입력데이터의 배치를 동일하게 해줌
과제1 
- randn
	- 정규분포에서 랜덤 실수 반환
- torch.nn.Linear(_in_features_, _out_features_, _bias=True_, _device=None_, _dtype=None_)
	- ax+b의 형태로 변환
		
- nn.Module
	- import torch.nn as nn
	- import torch.nn.functional as F
	- 상자
	- 이 안에 여러 함수를 담은 layer를 모아놓을 수 있다
	- Module안의 클래스는 지속적으로 상속받아 사용
- nn.sequential
	- 모듈을 묶어 순차적으로 실행

- nn.MModuleList
	- 파이썬의 list와 같다.
	- list를 넣을경우 submodule로 저장되지 않기 때문에 저장이 되지 않는다.

- buffer
	- ![[Pasted image 20231114213529.png]]
	- Parameter가 아닌 Tensor를 등록하여 모델저장시 Tensor를 같이 저장
	- 등록은 register_buffer(이름, 받는텐서)
	- 사용은 model.get_buffer(버퍼이름)
	
- children, modules
	- 모델 내부의 모듈을 확인하는 함수
	- iterator[module]로 반환
	- modules는 모든 서브모듈, children은 한 단계 아래의 서브모듈
	- 특정 모듈만 가져오고 싶으면 get_submodule(model, '모듈이름')
	-
- named_parameters()
	- parameter 확인

- named_buffers
	- 버퍼의 전체 목록
- Docstring
	- """ """으로 지정하면 알아서 model.__doc__이 생성된다고 이해함

![[Pasted image 20231114214823.png]]
- hook에 대한 개념
	- __dict__를 이용하면 확인 가능
	-  backward, forward의 전후로 작동하여 주어진 함수내의 값을 진행시킴.
	-  backward의 경우는 tensor 연산도 가능한 모양.
		
- postorder Traversal 방식
		1. 왼쪽 서브트리를 Postorder로 순회합니다.
		2. 오른쪽 서브트리를 Postorder로 순회합니다.
		3. 현재 노드를 방문합니다.
		![[Pasted image 20231114153313.png]]


# 4. 회고록
- 이번주는 파이토치 문서를 보면서 이를 수행하는 과제가 주를 이루고 있다. 배운것을 바로 적용하고 변형을 해가며 진행하는 과제이기 때문에 조금 많이 더딘것 같다. 시간이 날 때마다 다시 보면서 전체적인 구조를 확인하는 과정이 필수라고 생각한다. 처음에는 눈으로만 잠깐 읽히던 구조들이 같은 구조를 반복해서 봄에따라 점점 익숙해지고 다음에 어떤 코드가 나올지 조금씩 예상이 된다. 더 나아가면 예시를 보지않고 어느정도 머리속에서 구현이 가능할 것 같지만 지금은 아닌 것 같다. 그렇게 되기위해 더 많이 참고하고 봐야할 것 같다.